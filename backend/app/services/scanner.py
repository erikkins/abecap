"""
Scanner Service - Core trading signal generation

Implements the optimized DWAP strategy:
- Buy: Price > DWAP √ó 1.05
- Stop: 8%
- Target: 20%

Supports full NASDAQ + NYSE universe (~6000 stocks)
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from dataclasses import dataclass, asdict
import asyncio
import logging

try:
    import yfinance as yf
    YFINANCE_AVAILABLE = True
except ImportError:
    YFINANCE_AVAILABLE = False

from app.core.config import settings, get_universe
from app.services.stock_universe import EXCLUDED_PATTERNS

logger = logging.getLogger(__name__)

# Fast set lookup for excluded symbols (ETFs, leveraged products, etc.)
_EXCLUDED_SET = set(EXCLUDED_PATTERNS)


@dataclass
class SignalData:
    """Trading signal data (legacy DWAP)"""
    symbol: str
    signal_type: str
    price: float
    dwap: float
    pct_above_dwap: float
    volume: int
    volume_ratio: float
    stop_loss: float
    profit_target: float
    ma_50: float
    ma_200: float
    high_52w: float
    is_strong: bool
    timestamp: str
    # New fields for enhanced analysis
    signal_strength: float = 0.0  # 0-100 composite score
    sector: str = ""
    recommendation: str = ""

    def to_dict(self):
        return asdict(self)


@dataclass
class MomentumSignalData:
    """Momentum-based trading signal data (v2 strategy)"""
    symbol: str
    rank: int
    price: float
    short_momentum: float  # 10-day momentum %
    long_momentum: float   # 60-day momentum %
    volatility: float      # Annualized volatility %
    composite_score: float # Weighted score for ranking
    ma_20: float
    ma_50: float
    dist_from_50d_high: float  # % below 50-day high (0 = at high)
    passes_quality_filter: bool
    trailing_stop: float
    timestamp: str
    sector: str = ""
    recommendation: str = ""

    def to_dict(self):
        return asdict(self)


class ScannerService:
    """
    Stock scanner service

    Scans universe for DWAP-based buy signals.
    Supports dynamic universe from NASDAQ + NYSE.
    Auto-loads full universe from S3 cache on startup.
    """

    # Required symbols that are always fetched (for market regime, benchmark, etc.)
    REQUIRED_SYMBOLS = ['SPY', '^VIX']

    def __init__(self):
        # Start with config universe, will be replaced by full universe on load
        self.universe = get_universe()
        self.data_cache: Dict[str, pd.DataFrame] = {}
        self.last_scan: Optional[datetime] = None
        self.signals: List[SignalData] = []
        self.full_universe_loaded = False

    async def load_full_universe(self, max_cache_age_hours: int = 168):
        """
        Load full NASDAQ + NYSE universe from S3 cache or fetch fresh.

        Uses 7-day cache by default since the stock universe doesn't change frequently.
        This replaces the default 80-stock list with all tradeable stocks (~6500).

        Args:
            max_cache_age_hours: Max age of cached universe (default 168 = 7 days)
        """
        try:
            from app.services.stock_universe import stock_universe_service

            # Use ensure_loaded which checks cache first
            symbols = await stock_universe_service.ensure_loaded(max_cache_age_hours=max_cache_age_hours)
            if symbols and len(symbols) > 100:
                self.universe = symbols
                self.full_universe_loaded = True
                logger.info(f"Loaded full universe: {len(self.universe)} symbols")
            else:
                logger.warning(f"Universe load returned only {len(symbols) if symbols else 0} symbols, keeping default")
            return self.universe
        except Exception as e:
            logger.error(f"Failed to load full universe: {e}")
            return self.universe

    async def ensure_universe_loaded(self):
        """
        Ensure the full universe is loaded.
        Called on Lambda cold start.
        """
        if not self.full_universe_loaded or len(self.universe) < 100:
            await self.load_full_universe()
    
    # =========================================================================
    # INDICATORS
    # =========================================================================
    
    @staticmethod
    def dwap(prices: pd.Series, volumes: pd.Series, period: int = 200) -> pd.Series:
        """Daily Weighted Average Price"""
        pv = prices * volumes
        return pv.rolling(period, min_periods=50).sum() / volumes.rolling(period, min_periods=50).sum()
    
    @staticmethod
    def sma(series: pd.Series, period: int) -> pd.Series:
        """Simple Moving Average"""
        return series.rolling(period, min_periods=1).mean()
    
    @staticmethod
    def high_52w(prices: pd.Series) -> pd.Series:
        """52-week rolling high"""
        return prices.rolling(252, min_periods=1).max()

    @staticmethod
    def momentum(prices: pd.Series, period: int) -> pd.Series:
        """Price momentum (% change over period)"""
        return (prices / prices.shift(period) - 1) * 100

    @staticmethod
    def volatility(prices: pd.Series, period: int = 20) -> pd.Series:
        """Annualized volatility"""
        returns = prices.pct_change()
        return returns.rolling(period).std() * np.sqrt(252) * 100

    @staticmethod
    def distance_from_high(prices: pd.Series, period: int = 50) -> pd.Series:
        """% below rolling high (0 = at high, negative = below)"""
        rolling_high = prices.rolling(period, min_periods=1).max()
        return (prices / rolling_high - 1) * 100
    
    # =========================================================================
    # DATA FETCHING
    # =========================================================================

    async def fetch_data(self, symbols: List[str] = None, period: str = "5y") -> Dict[str, pd.DataFrame]:
        """
        Fetch historical data from yfinance with throttled batch requests.
        This fetches FULL historical data - use fetch_incremental() for daily updates.

        Args:
            symbols: List of tickers (default: full universe)
            period: Data period (1y, 2y, 5y, max)

        Returns:
            Dict mapping symbol to DataFrame with indicators
        """
        if not YFINANCE_AVAILABLE:
            raise RuntimeError("yfinance not installed")

        symbols = symbols or self.universe

        # Always include required symbols (SPY for benchmark, ^VIX for market regime)
        symbols_set = set(symbols)
        for req in self.REQUIRED_SYMBOLS:
            if req not in symbols_set:
                symbols = list(symbols) + [req]
                symbols_set.add(req)

        # Batch settings to avoid rate limiting
        BATCH_SIZE = 10  # Fetch 10 stocks at a time
        DELAY_BETWEEN_BATCHES = 1.5  # Seconds between batches
        MAX_RETRIES = 2

        total = len(symbols)
        successful = 0
        failed = []

        print(f"üìä Fetching data for {total} symbols in batches of {BATCH_SIZE}...")

        # Process in batches
        for i in range(0, total, BATCH_SIZE):
            batch = symbols[i:i + BATCH_SIZE]
            batch_num = i // BATCH_SIZE + 1
            total_batches = (total + BATCH_SIZE - 1) // BATCH_SIZE

            for retry in range(MAX_RETRIES):
                try:
                    # Run yfinance in thread pool (it's blocking)
                    loop = asyncio.get_event_loop()
                    data = await loop.run_in_executor(
                        None,
                        lambda b=batch: yf.download(
                            b,
                            period=period,
                            progress=False,
                            threads=True,
                            timeout=30
                        )
                    )

                    # Process each symbol in batch
                    for symbol in batch:
                        try:
                            if len(batch) > 1:
                                df = pd.DataFrame({
                                    'date': data.index,
                                    'open': data['Open'][symbol],
                                    'high': data['High'][symbol],
                                    'low': data['Low'][symbol],
                                    'close': data['Close'][symbol],
                                    'volume': data['Volume'][symbol],
                                }).dropna()
                            else:
                                df = pd.DataFrame({
                                    'date': data.index,
                                    'open': data['Open'],
                                    'high': data['High'],
                                    'low': data['Low'],
                                    'close': data['Close'],
                                    'volume': data['Volume'],
                                }).dropna()

                            if len(df) < 50:  # Skip if not enough data
                                continue

                            df['date'] = pd.to_datetime(df['date'])
                            df = df.set_index('date').sort_index()

                            # Compute indicators
                            df['dwap'] = self.dwap(df['close'], df['volume'])
                            df['ma_50'] = self.sma(df['close'], 50)
                            df['ma_200'] = self.sma(df['close'], 200)
                            df['vol_avg'] = self.sma(df['volume'], 200)
                            df['high_52w'] = self.high_52w(df['close'])

                            self.data_cache[symbol] = df
                            successful += 1

                        except Exception as e:
                            if symbol not in failed:
                                failed.append(symbol)

                    # Success - break retry loop
                    break

                except Exception as e:
                    if retry < MAX_RETRIES - 1:
                        print(f"  Batch {batch_num} failed, retrying... ({e})")
                        await asyncio.sleep(DELAY_BETWEEN_BATCHES * 2)
                    else:
                        print(f"  Batch {batch_num} failed after {MAX_RETRIES} retries")
                        failed.extend(batch)

            # Progress update every 10 batches
            if batch_num % 10 == 0 or batch_num == total_batches:
                print(f"  Progress: {batch_num}/{total_batches} batches ({successful} symbols loaded)")

            # Delay between batches to avoid rate limiting
            if i + BATCH_SIZE < total:
                await asyncio.sleep(DELAY_BETWEEN_BATCHES)

        print(f"‚úÖ Loaded {successful}/{total} symbols ({len(failed)} failed)")

        return self.data_cache

    async def fetch_incremental(self, symbols: List[str] = None) -> Dict[str, int]:
        """
        Fetch only NEW data since the last cached date for each symbol.
        This is the efficient daily update - only gets today's prices.

        Args:
            symbols: List of tickers (default: all symbols in cache)

        Returns:
            Dict with counts: {updated: N, failed: N, skipped: N}
        """
        if not YFINANCE_AVAILABLE:
            raise RuntimeError("yfinance not installed")

        # Use symbols from cache if not specified
        symbols = symbols or list(self.data_cache.keys())
        if not symbols:
            logger.warning("No symbols to update - cache is empty")
            return {"updated": 0, "failed": 0, "skipped": 0}

        from datetime import timedelta

        BATCH_SIZE = 100  # Large batches OK for incremental (only a few days of data)
        DELAY_BETWEEN_BATCHES = 0.5

        updated = 0
        failed = 0
        skipped = 0

        today = pd.Timestamp.now().normalize().tz_localize(None)

        logger.info(f"üìä Incremental update for {len(symbols)} symbols...")

        # Process in batches
        for i in range(0, len(symbols), BATCH_SIZE):
            batch = symbols[i:i + BATCH_SIZE]

            # Find the oldest "last date" in this batch to determine fetch period
            oldest_last_date = today
            for symbol in batch:
                if symbol in self.data_cache and len(self.data_cache[symbol]) > 0:
                    last_date = self.data_cache[symbol].index.max()
                    # Strip tz for safe comparison (cache may be tz-aware or naive)
                    if hasattr(last_date, 'tz') and last_date.tz is not None:
                        last_date = last_date.tz_localize(None)
                    if last_date < oldest_last_date:
                        oldest_last_date = last_date

            # If all data is from today, skip this batch
            if oldest_last_date >= today - timedelta(days=1):
                skipped += len(batch)
                continue

            # Fetch from oldest_last_date to now (add buffer for safety)
            start_date = (oldest_last_date - timedelta(days=5)).strftime('%Y-%m-%d')

            try:
                loop = asyncio.get_event_loop()
                data = await loop.run_in_executor(
                    None,
                    lambda b=batch, s=start_date: yf.download(
                        b,
                        start=s,
                        progress=False,
                        threads=True,
                        timeout=30
                    )
                )

                if data.empty:
                    skipped += len(batch)
                    continue

                # Process each symbol
                for symbol in batch:
                    try:
                        if symbol not in self.data_cache:
                            skipped += 1
                            continue

                        # Extract new data for this symbol
                        if len(batch) > 1:
                            new_df = pd.DataFrame({
                                'date': data.index,
                                'open': data['Open'][symbol],
                                'high': data['High'][symbol],
                                'low': data['Low'][symbol],
                                'close': data['Close'][symbol],
                                'volume': data['Volume'][symbol],
                            }).dropna()
                        else:
                            new_df = pd.DataFrame({
                                'date': data.index,
                                'open': data['Open'],
                                'high': data['High'],
                                'low': data['Low'],
                                'close': data['Close'],
                                'volume': data['Volume'],
                            }).dropna()

                        if new_df.empty:
                            skipped += 1
                            continue

                        new_df['date'] = pd.to_datetime(new_df['date'])
                        new_df = new_df.set_index('date').sort_index()

                        # Get existing data
                        existing_df = self.data_cache[symbol]
                        last_cached_date = existing_df.index.max()

                        # Filter to only truly new rows
                        new_rows = new_df[new_df.index > last_cached_date]

                        if new_rows.empty:
                            skipped += 1
                            continue

                        # Append new rows to existing data
                        combined = pd.concat([existing_df[['open', 'high', 'low', 'close', 'volume']],
                                            new_rows[['open', 'high', 'low', 'close', 'volume']]])
                        combined = combined[~combined.index.duplicated(keep='last')]
                        combined = combined.sort_index()

                        # Recompute indicators on full dataset
                        combined['dwap'] = self.dwap(combined['close'], combined['volume'])
                        combined['ma_50'] = self.sma(combined['close'], 50)
                        combined['ma_200'] = self.sma(combined['close'], 200)
                        combined['vol_avg'] = self.sma(combined['volume'], 200)
                        combined['high_52w'] = self.high_52w(combined['close'])

                        self.data_cache[symbol] = combined
                        updated += 1

                    except Exception as e:
                        logger.debug(f"Failed to update {symbol}: {e}")
                        failed += 1

            except Exception as e:
                logger.error(f"Batch fetch failed: {e}")
                failed += len(batch)

            # Small delay between batches
            if i + BATCH_SIZE < len(symbols):
                await asyncio.sleep(DELAY_BETWEEN_BATCHES)

        logger.info(f"‚úÖ Incremental update complete: {updated} updated, {skipped} skipped, {failed} failed")

        return {"updated": updated, "failed": failed, "skipped": skipped}
    
    # =========================================================================
    # SIGNAL GENERATION
    # =========================================================================
    
    def _ensure_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """Compute indicators if missing from DataFrame (lazy computation)"""
        if 'dwap' not in df.columns:
            df = df.copy()
            df['dwap'] = self.dwap(df['close'], df['volume'])
            df['ma_50'] = self.sma(df['close'], 50)
            df['ma_200'] = self.sma(df['close'], 200)
            df['vol_avg'] = self.sma(df['volume'], 200)
            df['high_52w'] = self.high_52w(df['close'])
        return df

    def _ensure_momentum_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """Compute momentum indicators if missing (lazy computation)"""
        if 'short_mom' not in df.columns:
            df = df.copy()
            df['short_mom'] = self.momentum(df['close'], settings.SHORT_MOMENTUM_DAYS)
            df['long_mom'] = self.momentum(df['close'], settings.LONG_MOMENTUM_DAYS)
            df['volatility'] = self.volatility(df['close'], 20)
            df['ma_20'] = self.sma(df['close'], 20)
            df['ma_50'] = self.sma(df['close'], 50)
            df['dist_from_50d_high'] = self.distance_from_high(df['close'], 50)
        return df

    def rank_stocks_momentum(self, apply_market_filter: bool = True, as_of_date=None) -> List[MomentumSignalData]:
        """
        Rank all stocks by momentum composite score (v2 strategy)

        Scoring: short_mom * 0.5 + long_mom * 0.3 - volatility * 0.2

        Quality filters:
        - Price > MA20 and MA50 (uptrend)
        - Within 5% of 50-day high (breakout zone)
        - Volume > MIN_VOLUME
        - Price > MIN_PRICE

        Returns:
            List of MomentumSignalData sorted by composite score (highest first)
        """
        # Check market regime if enabled
        if apply_market_filter and settings.MARKET_FILTER_ENABLED:
            try:
                from app.services.market_analysis import market_analysis_service
                market_state = market_analysis_service.get_market_state()
                if market_state and not market_state.get('spy_above_200ma', True):
                    logger.info("Market filter: SPY below 200MA, returning empty signals")
                    return []
            except Exception as e:
                logger.warning(f"Market filter check failed: {e}")

        candidates = []

        for symbol in self.data_cache:
            if symbol in _EXCLUDED_SET:
                continue

            df = self.data_cache[symbol]
            if len(df) < settings.LONG_MOMENTUM_DAYS + 20:
                continue

            # Ensure momentum indicators are computed
            df = self._ensure_momentum_indicators(df)
            self.data_cache[symbol] = df

            # Time-travel: truncate to as_of_date after indicators computed on full df
            if as_of_date:
                as_of_ts = pd.Timestamp(as_of_date).normalize()
                if hasattr(df.index, 'tz') and df.index.tz is not None:
                    as_of_ts = as_of_ts.tz_localize(df.index.tz)
                df = df[df.index <= as_of_ts]
                if len(df) < settings.LONG_MOMENTUM_DAYS + 20:
                    continue

            row = df.iloc[-1]
            price = row['close']
            volume = row.get('volume', 0)
            short_mom = row.get('short_mom', np.nan)
            long_mom = row.get('long_mom', np.nan)
            vol = row.get('volatility', np.nan)
            ma_20 = row.get('ma_20', 0)
            ma_50 = row.get('ma_50', 0)
            dist_from_high = row.get('dist_from_50d_high', -100)

            # Skip invalid data
            if pd.isna(short_mom) or pd.isna(long_mom) or pd.isna(vol):
                continue

            # Apply basic filters
            if price < settings.MIN_PRICE or volume < settings.MIN_VOLUME:
                continue

            # Quality filter: uptrend (price > MA20 and MA50)
            passes_trend = price > ma_20 > 0 and price > ma_50 > 0

            # Quality filter: near 50-day high (within NEAR_50D_HIGH_PCT)
            passes_breakout = dist_from_high >= -settings.NEAR_50D_HIGH_PCT

            passes_quality = passes_trend and passes_breakout

            # Calculate composite score
            composite_score = (
                short_mom * settings.SHORT_MOM_WEIGHT +
                long_mom * settings.LONG_MOM_WEIGHT -
                vol * settings.VOLATILITY_PENALTY
            )

            # Calculate trailing stop
            trailing_stop = price * (1 - settings.TRAILING_STOP_PCT / 100)

            signal_ts = str(as_of_date)[:10] if as_of_date else datetime.now().isoformat()
            candidates.append(MomentumSignalData(
                symbol=symbol,
                rank=0,  # Will be set after sorting
                price=round(price, 2),
                short_momentum=round(short_mom, 2),
                long_momentum=round(long_mom, 2),
                volatility=round(vol, 2),
                composite_score=round(composite_score, 2),
                ma_20=round(ma_20, 2),
                ma_50=round(ma_50, 2),
                dist_from_50d_high=round(dist_from_high, 2),
                passes_quality_filter=passes_quality,
                trailing_stop=round(trailing_stop, 2),
                timestamp=signal_ts
            ))

        # Sort by composite score (highest first), prioritizing quality filter pass
        candidates.sort(key=lambda x: (not x.passes_quality_filter, -x.composite_score))

        # Apply sector cap ‚Äî max N stocks per sector to prevent concentration
        # Only cap sectors we actually have data for; skip cap for unknown sectors
        from app.services.stock_universe import stock_universe_service
        sector_counts: Dict[str, int] = {}
        capped = []
        for c in candidates:
            info = stock_universe_service.get_symbol_info(c.symbol)
            sector = (info.get('sector', '') if info else '') or ''
            c.sector = sector
            if not sector:
                capped.append(c)
            else:
                count = sector_counts.get(sector, 0)
                if count < settings.MOMENTUM_SECTOR_CAP:
                    capped.append(c)
                    sector_counts[sector] = count + 1
        candidates = capped

        # Assign ranks
        for i, c in enumerate(candidates):
            c.rank = i + 1

        logger.info(f"Momentum ranking complete: {len(candidates)} candidates, "
                   f"{sum(1 for c in candidates if c.passes_quality_filter)} pass quality filter")

        return candidates

    def analyze_stock(self, symbol: str, as_of_date=None) -> Optional[SignalData]:
        """
        Analyze single stock for buy signals

        Buy conditions:
        - Price > DWAP √ó (1 + threshold)
        - Volume > minimum
        - Price > minimum

        Strong signal (bonus):
        - Volume > avg √ó spike_mult
        - Price > MA50 > MA200 (healthy trend)
        """
        if symbol not in self.data_cache:
            return None

        df = self.data_cache[symbol]
        if len(df) < 200:
            return None

        # Ensure indicators are computed (lazy computation)
        df = self._ensure_indicators(df)
        self.data_cache[symbol] = df  # Cache the computed indicators

        # Time-travel: truncate to as_of_date after indicators computed on full df
        if as_of_date:
            as_of_ts = pd.Timestamp(as_of_date).normalize()
            if hasattr(df.index, 'tz') and df.index.tz is not None:
                as_of_ts = as_of_ts.tz_localize(df.index.tz)
            df = df[df.index <= as_of_ts]
            if len(df) < 200:
                return None

        # Current values
        row = df.iloc[-1]
        price = row['close']
        volume = row['volume']
        current_dwap = row.get('dwap', np.nan)
        vol_avg = row.get('vol_avg', 1)
        ma_50 = row.get('ma_50', 0)
        ma_200 = row.get('ma_200', 0)
        h_52w = row.get('high_52w', price)
        
        # Skip if DWAP invalid
        if pd.isna(current_dwap) or current_dwap <= 0:
            return None
        
        # Calculate metrics
        pct_above_dwap = (price / current_dwap - 1) * 100
        vol_ratio = volume / vol_avg if vol_avg > 0 else 0
        
        # Check buy conditions
        is_buy = (
            pct_above_dwap >= settings.DWAP_THRESHOLD_PCT and
            volume >= settings.MIN_VOLUME and
            price >= settings.MIN_PRICE
        )
        
        if not is_buy:
            return None
        
        # Strong signal check
        is_strong = (
            vol_ratio >= settings.VOLUME_SPIKE_MULT and
            price > ma_50 > ma_200
        )
        
        # Calculate stops/targets
        stop_loss = price * (1 - settings.STOP_LOSS_PCT / 100)
        profit_target = price * (1 + settings.PROFIT_TARGET_PCT / 100)
        
        signal_ts = str(as_of_date)[:10] if as_of_date else datetime.now().isoformat()
        return SignalData(
            symbol=symbol,
            signal_type='BUY',
            price=round(price, 2),
            dwap=round(current_dwap, 2),
            pct_above_dwap=round(pct_above_dwap, 1),
            volume=int(volume),
            volume_ratio=round(vol_ratio, 2),
            stop_loss=round(stop_loss, 2),
            profit_target=round(profit_target, 2),
            ma_50=round(ma_50, 2),
            ma_200=round(ma_200, 2),
            high_52w=round(h_52w, 2),
            is_strong=is_strong,
            timestamp=signal_ts
        )
    
    async def scan(
        self,
        refresh_data: bool = True,
        apply_market_filter: bool = True,
        min_signal_strength: float = 0,
        as_of_date=None
    ) -> List[SignalData]:
        """
        Run full market scan with market regime awareness

        Args:
            refresh_data: Whether to fetch fresh data
            apply_market_filter: Apply market regime filtering
            min_signal_strength: Minimum signal strength to include (0-100)

        Returns:
            List of SignalData objects sorted by signal strength
        """
        # Time-travel mode: never refresh data (we need historical data intact)
        if as_of_date:
            refresh_data = False

        # In Lambda mode, always try to load from S3 cache first (faster than yfinance)
        import os
        is_lambda = os.environ.get("ENVIRONMENT") == "prod"

        print(f"üîç Scan called: is_lambda={is_lambda}, data_cache_size={len(self.data_cache)}, refresh_data={refresh_data}")

        if not self.data_cache and is_lambda:
            try:
                from app.services.data_export import data_export_service
                print("üì• Lambda cold start - loading price data from S3...")
                cached_data = data_export_service.import_all()
                if cached_data:
                    self.data_cache = cached_data
                    print(f"‚úÖ Loaded {len(cached_data)} symbols from S3 cache")
                else:
                    print("‚ö†Ô∏è S3 import returned empty data")
            except Exception as e:
                import traceback
                print(f"‚ùå Failed to load from S3: {e}")
                print(traceback.format_exc())

        # In Lambda mode with S3 data, skip yfinance refresh (use cached data)
        # In local mode or if no S3 data, fetch from yfinance
        should_fetch = (refresh_data and not is_lambda) or not self.data_cache
        if should_fetch:
            await self.fetch_data()

        # Update market analysis and check regime
        market_available = False
        market_favorable = True  # Default to favorable if we can't check

        try:
            from app.services.market_analysis import market_analysis_service
            await market_analysis_service.update_market_state()
            await market_analysis_service.update_sector_strength()
            market_available = True

            # Check if SPY is above 200-day MA (market filter)
            if settings.MARKET_FILTER_ENABLED:
                market_state = market_analysis_service.get_market_state()
                if market_state:
                    market_favorable = market_state.get('spy_above_200ma', True)
                    if not market_favorable:
                        logger.info("Market filter active: SPY below 200MA, returning empty signals")
                        return []
        except Exception as e:
            logger.warning(f"Market analysis unavailable: {e}")

        self.signals = []

        for symbol in self.data_cache:
            if symbol in _EXCLUDED_SET:
                continue
            signal = self.analyze_stock(symbol, as_of_date=as_of_date)
            if not signal:
                continue

            # Calculate signal strength
            if market_available:
                signal.signal_strength = market_analysis_service.calculate_signal_strength(
                    pct_above_dwap=signal.pct_above_dwap,
                    volume_ratio=signal.volume_ratio,
                    is_strong=signal.is_strong,
                    sector=signal.sector
                )

                # Apply market regime filter
                if apply_market_filter:
                    should_take, reason = market_analysis_service.should_take_signal(signal.signal_strength)
                    signal.recommendation = reason
                    if not should_take:
                        continue
            else:
                # Default strength calculation without market data
                signal.signal_strength = (
                    min(signal.pct_above_dwap * 5, 30) +
                    min((signal.volume_ratio - 1) * 15, 30) +
                    (25 if signal.is_strong else 0)
                )
                signal.recommendation = "Market data unavailable"

            # Apply minimum strength filter
            if signal.signal_strength < min_signal_strength:
                continue

            self.signals.append(signal)

        # Sort by signal strength (highest first)
        self.signals.sort(key=lambda s: -s.signal_strength)

        self.last_scan = datetime.now()

        logger.info(f"Scan complete: {len(self.signals)} signals found")

        return self.signals
    
    def get_strong_signals(self) -> List[SignalData]:
        """Get only strong signals"""
        return [s for s in self.signals if s.is_strong]
    
    def get_watchlist(self, threshold: float = 3.0) -> List[SignalData]:
        """Get stocks approaching DWAP threshold"""
        watchlist = []
        
        for symbol, df in self.data_cache.items():
            if len(df) < 200:
                continue
            
            row = df.iloc[-1]
            price = row['close']
            dwap = row['dwap']
            
            if pd.isna(dwap) or dwap <= 0:
                continue
            
            pct_above = (price / dwap - 1) * 100
            
            # Approaching but not yet at threshold
            if threshold <= pct_above < settings.DWAP_THRESHOLD_PCT:
                watchlist.append({
                    'symbol': symbol,
                    'price': round(price, 2),
                    'dwap': round(dwap, 2),
                    'pct_above_dwap': round(pct_above, 1)
                })
        
        return sorted(watchlist, key=lambda x: -x['pct_above_dwap'])


    def generate_sell_signals(
        self,
        positions: List[dict],
        regime_forecast=None,
        trailing_stop_pct: float = 12.0,
    ) -> List[dict]:
        """
        For each position, determine: HOLD, WARNING, or SELL.

        Args:
            positions: List of position dicts with at least symbol, entry_price, shares,
                       and optionally highest_price (tracked high water mark)
            regime_forecast: RegimeForecast from market_regime.py (optional)
            trailing_stop_pct: Trailing stop percentage (default 12%)

        Returns:
            List of position dicts with added sell guidance fields.
        """
        results = []

        for pos in positions:
            symbol = pos.get('symbol', '')
            entry_price = pos.get('entry_price', 0)
            stored_highest = pos.get('highest_price', entry_price)

            # Look up current price
            current_price = None
            if symbol in self.data_cache and len(self.data_cache[symbol]) > 0:
                current_price = float(self.data_cache[symbol].iloc[-1]['close'])
            else:
                current_price = pos.get('current_price', entry_price)

            # Calculate high water mark
            high_water_mark = max(entry_price, stored_highest or entry_price, current_price or entry_price)

            # Calculate trailing stop
            trailing_stop_level = high_water_mark * (1 - trailing_stop_pct / 100)
            distance_to_stop_pct = (
                (current_price - trailing_stop_level) / trailing_stop_level * 100
                if trailing_stop_level > 0 else 100
            )

            # P&L from entry
            pnl_pct = ((current_price - entry_price) / entry_price * 100) if entry_price > 0 else 0

            # Determine action
            action = "hold"
            action_reason = ""

            # Check regime-based exits first
            if regime_forecast:
                rec = regime_forecast.recommended_action
                if rec == "go_to_cash":
                    action = "sell"
                    action_reason = f"Market regime exit ‚Äî {regime_forecast.outlook_detail}"
                elif rec == "reduce_exposure":
                    action = "warning"
                    action_reason = f"Regime deteriorating ‚Äî consider reducing exposure"
                elif rec == "tighten_stops":
                    if action != "sell":
                        action = "warning"
                        action_reason = f"Tighten stops ‚Äî {regime_forecast.outlook_detail}"

            # Check trailing stop (overrides regime warning if triggered)
            if current_price <= trailing_stop_level:
                action = "sell"
                action_reason = f"Trailing stop hit ‚Äî price ${current_price:.2f} below stop ${trailing_stop_level:.2f}"

            # Check proximity to trailing stop (warning zone)
            elif distance_to_stop_pct < 3 and action != "sell":
                action = "warning"
                if not action_reason:
                    action_reason = f"Within {distance_to_stop_pct:.1f}% of trailing stop ${trailing_stop_level:.2f}"

            # Default hold reason
            if action == "hold" and not action_reason:
                action_reason = f"Trailing stop at ${trailing_stop_level:.2f} ({distance_to_stop_pct:.0f}% away)"

            result = {
                **pos,
                'current_price': round(current_price, 2),
                'action': action,
                'action_reason': action_reason,
                'trailing_stop_level': round(trailing_stop_level, 2),
                'distance_to_stop_pct': round(distance_to_stop_pct, 1),
                'high_water_mark': round(high_water_mark, 2),
                'pnl_pct': round(pnl_pct, 1),
            }
            results.append(result)

        return results


# Singleton instance
scanner_service = ScannerService()
